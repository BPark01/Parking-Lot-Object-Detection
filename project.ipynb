{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "#!pip install torch\n",
    "#!pip install torchvision\n",
    "#!pip install imgaug\n",
    "import torch      #pytorch\n",
    "import torch.nn as nn     #pytorch network\n",
    "from torch.utils.data import Dataset, DataLoader      #pytorch dataset\n",
    "from torch.utils.tensorboard import SummaryWriter     #tensorboard\n",
    "import torchvision      #torchvision\n",
    "import torch.optim as optim     #pytorch optimizer\n",
    "import numpy as np      #numpy\n",
    "import matplotlib.pyplot as plt     #matplotlib(이미지 표시를 위해 필요)\n",
    "from collections import OrderedDict     #python라이브러리 (라벨 dictionary를 만들 때 필요)\n",
    "import os     #os\n",
    "import xml.etree.ElementTree as Et      #Pascal xml을 읽어올 때 필요\n",
    "from xml.etree.ElementTree import Element, ElementTree\n",
    "import cv2      #opencv (box 그리기를 할 때 필요)\n",
    "from PIL import Image     #PILLOW (이미지 읽기)\n",
    "import time     #time\n",
    "import imgaug as ia     #imgaug\n",
    "from imgaug import augmenters as iaa\n",
    "from torchvision import transforms      #torchvision transform\n",
    "import pandas as pd\n",
    "#GPU연결\n",
    "if torch.cuda.is_available():\n",
    "  device = torch.device('cuda:0')\n",
    "else:\n",
    "  device = torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import CLASS_NAME_TO_ID, visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './DRIVING-DATASET/Detection/' # 안에 image, train, val, df.csv가 있음 데이터가 나눠지면 수정예정\n",
    "data_df = pd.read_csv('all_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>center_x</th>\n",
       "      <th>center_y</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>angle</th>\n",
       "      <th>occupied</th>\n",
       "      <th>x_max</th>\n",
       "      <th>x_min</th>\n",
       "      <th>y_max</th>\n",
       "      <th>y_min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-09-11_15_16_58_1</td>\n",
       "      <td>300</td>\n",
       "      <td>207</td>\n",
       "      <td>55</td>\n",
       "      <td>32</td>\n",
       "      <td>-74</td>\n",
       "      <td>1.0</td>\n",
       "      <td>324</td>\n",
       "      <td>278</td>\n",
       "      <td>230</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-09-11_15_16_58_2</td>\n",
       "      <td>332</td>\n",
       "      <td>209</td>\n",
       "      <td>56</td>\n",
       "      <td>33</td>\n",
       "      <td>-77</td>\n",
       "      <td>0.0</td>\n",
       "      <td>355</td>\n",
       "      <td>310</td>\n",
       "      <td>233</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-09-11_15_16_58_3</td>\n",
       "      <td>366</td>\n",
       "      <td>208</td>\n",
       "      <td>52</td>\n",
       "      <td>32</td>\n",
       "      <td>-77</td>\n",
       "      <td>1.0</td>\n",
       "      <td>388</td>\n",
       "      <td>345</td>\n",
       "      <td>233</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-09-11_15_16_58_4</td>\n",
       "      <td>398</td>\n",
       "      <td>207</td>\n",
       "      <td>54</td>\n",
       "      <td>36</td>\n",
       "      <td>-79</td>\n",
       "      <td>0.0</td>\n",
       "      <td>421</td>\n",
       "      <td>375</td>\n",
       "      <td>232</td>\n",
       "      <td>184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-09-11_15_16_58_5</td>\n",
       "      <td>430</td>\n",
       "      <td>210</td>\n",
       "      <td>50</td>\n",
       "      <td>31</td>\n",
       "      <td>-75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>452</td>\n",
       "      <td>409</td>\n",
       "      <td>232</td>\n",
       "      <td>187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>2012-09-11_15_16_58_96</td>\n",
       "      <td>441</td>\n",
       "      <td>530</td>\n",
       "      <td>67</td>\n",
       "      <td>53</td>\n",
       "      <td>-77</td>\n",
       "      <td>0.0</td>\n",
       "      <td>475</td>\n",
       "      <td>408</td>\n",
       "      <td>561</td>\n",
       "      <td>504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>2012-09-11_15_16_58_97</td>\n",
       "      <td>494</td>\n",
       "      <td>537</td>\n",
       "      <td>70</td>\n",
       "      <td>54</td>\n",
       "      <td>-79</td>\n",
       "      <td>0.0</td>\n",
       "      <td>527</td>\n",
       "      <td>461</td>\n",
       "      <td>567</td>\n",
       "      <td>507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>2012-09-11_15_16_58_98</td>\n",
       "      <td>549</td>\n",
       "      <td>534</td>\n",
       "      <td>64</td>\n",
       "      <td>55</td>\n",
       "      <td>-82</td>\n",
       "      <td>0.0</td>\n",
       "      <td>581</td>\n",
       "      <td>518</td>\n",
       "      <td>563</td>\n",
       "      <td>504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>2012-09-11_15_16_58_99</td>\n",
       "      <td>602</td>\n",
       "      <td>536</td>\n",
       "      <td>64</td>\n",
       "      <td>47</td>\n",
       "      <td>-85</td>\n",
       "      <td>0.0</td>\n",
       "      <td>629</td>\n",
       "      <td>576</td>\n",
       "      <td>567</td>\n",
       "      <td>505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>2012-09-11_15_16_58_100</td>\n",
       "      <td>653</td>\n",
       "      <td>538</td>\n",
       "      <td>65</td>\n",
       "      <td>54</td>\n",
       "      <td>-86</td>\n",
       "      <td>1.0</td>\n",
       "      <td>683</td>\n",
       "      <td>624</td>\n",
       "      <td>570</td>\n",
       "      <td>507</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   filename  center_x  center_y  width  height  angle  \\\n",
       "0     2012-09-11_15_16_58_1       300       207     55      32    -74   \n",
       "1     2012-09-11_15_16_58_2       332       209     56      33    -77   \n",
       "2     2012-09-11_15_16_58_3       366       208     52      32    -77   \n",
       "3     2012-09-11_15_16_58_4       398       207     54      36    -79   \n",
       "4     2012-09-11_15_16_58_5       430       210     50      31    -75   \n",
       "..                      ...       ...       ...    ...     ...    ...   \n",
       "95   2012-09-11_15_16_58_96       441       530     67      53    -77   \n",
       "96   2012-09-11_15_16_58_97       494       537     70      54    -79   \n",
       "97   2012-09-11_15_16_58_98       549       534     64      55    -82   \n",
       "98   2012-09-11_15_16_58_99       602       536     64      47    -85   \n",
       "99  2012-09-11_15_16_58_100       653       538     65      54    -86   \n",
       "\n",
       "    occupied  x_max  x_min  y_max  y_min  \n",
       "0        1.0    324    278    230    185  \n",
       "1        0.0    355    310    233    185  \n",
       "2        1.0    388    345    233    185  \n",
       "3        0.0    421    375    232    184  \n",
       "4        1.0    452    409    232    187  \n",
       "..       ...    ...    ...    ...    ...  \n",
       "95       0.0    475    408    561    504  \n",
       "96       0.0    527    461    567    507  \n",
       "97       0.0    581    518    563    504  \n",
       "98       0.0    629    576    567    505  \n",
       "99       1.0    683    624    570    507  \n",
       "\n",
       "[100 rows x 11 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df[data_df['filename'].str.contains('2012-09-11_15_16_58')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = os.path.join('../DRIVING-DATASET/Detection/images/', image_file) # 모든 jpg 파일 중 첫번째 파일의 path를 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Detection_dataset() :\n",
    "    def init(self, data_dir, phase, transformer = None) :\n",
    "        self.data_dir = data_dir\n",
    "        self.phase = phase\n",
    "        self.transformer = transformer\n",
    "        self.data_df = pd.read_csv(os.path.join(self.data_dir, 'all_data.csv'))\n",
    "        self.image_files = [fn for fn in os.listdir(os.path.join(self.data_dir, phase)) if fn.endswith('jpg')]\n",
    "\n",
    "    def len(self) :\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def getitem(self, index) :\n",
    "        filename, image = self.get_image(index) # index의 이미지 파일 제목, 이미지\n",
    "        bboxes, class_ids = self.get_label(filename) # ['XMin', 'YMin', 'XMax', 'YMax'], class_ids\n",
    "\n",
    "        if self.transformer :\n",
    "            image = self.transformer(image)\n",
    "\n",
    "        target = {}\n",
    "        target['boxes'] = torch.Tensor(bboxes).float()\n",
    "        target['labels'] = torch.Tensor(class_ids).long()\n",
    "        return image, target, filename # target에는 박스 좌표와 라벨이 들어가 있음\n",
    "\n",
    "    def get_image(self, index) :\n",
    "        filename = self.image_files[index]\n",
    "        image_path = os.path.join(self.data_dir, self.phase, filename)\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        return filename, image\n",
    "\n",
    "    def get_label(self, filename) :\n",
    "        image_id = filename.split('.')[0]\n",
    "        meta_data = self.data_df[self.data_df['filename'].str.contains(image_id)]\n",
    "        class_ids = meta_data['occupied'].values\n",
    "        bboxes = meta_data[['x_min', 'y_min', 'x_max', 'y_max']].values\n",
    "        return bboxes, class_ids # 하나의 사진 안에 들어있는 바운딩 박스(들)의 좌표와 라벨(1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './DRIVING-DATASET/Detection/'\n",
    "dataset = Detection_dataset(data_dir = data_dir, phase = 'train', transformer=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 448 \n",
    "\n",
    "transformer = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize(size = (IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './DRIVING-DATASET/Detection/'\n",
    "transformed_dataset = Detection_dataset(data_dir=data_dir, phase = 'train', transformer=transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import make_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_image = make_grid(image, normalize = True).cpu().permute(1, 2, 0).numpy()\n",
    "\n",
    "boxes = target['boxes'].numpy() \n",
    "class_ids = target['labels'].numpy() \n",
    "\n",
    "n_obj = boxes.shape[0] \n",
    "bboxes = np.zeros(shape = (n_obj, 4), dtype = np.float32) \n",
    "bboxes[:, 0:2] = (boxes[:, 0:2] + boxes[:, 2:4]) / 2 \n",
    "bboxes[:, 2:4] = boxes[:, 2:4] - boxes[:, 0:2]\n",
    "\n",
    "canvas = visualize(np_image, bboxes, class_ids)\n",
    "\n",
    "plt.figure(figsize = (6, 6))\n",
    "plt.imshow(canvas)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch) :\n",
    "    image_list = []\n",
    "    target_list = []\n",
    "    filename_list = []\n",
    "    \n",
    "    for a, b, c in batch :\n",
    "        image_list.append(a)\n",
    "        target_list.append(b)\n",
    "        filename_list.append(c)\n",
    "    \n",
    "    return image_list, target_list, filename_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './DRIVING-DATASET/Detection'\n",
    "BATCH_SIZE = 6\n",
    "\n",
    "trainset = Detection_dataset(data_dir = data_dir, phase = 'train', transformer = transformer)\n",
    "trainloader = DataLoader(trainset, batch_size = BATCH_SIZE, shuffle = True, collate_fn = collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, batch in enumerate(trainloader) :\n",
    "    images = batch[0]\n",
    "    targets = batch[1]\n",
    "    filnames = batch[2]\n",
    "    \n",
    "    if index == 0 :\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataloader(data_dir, batch_size = 4, image_size = 448) :\n",
    "    dataloaders = {}\n",
    "    transformer = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize(size = (image_size, image_size)),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    train_dataset = Detection_dataset(data_dir, 'train', transformer)\n",
    "    dataloaders['train'] = DataLoader(train_dataset, batch_size = batch_size, shuffle = True, collate_fn = collate_fn)\n",
    "    \n",
    "    val_dataset = Detection_dataset(data_dir, 'val', transformer)\n",
    "    dataloaders['val'] = DataLoader(val_dataset, batch_size = batch_size, shuffle = True, collate_fn = collate_fn)\n",
    "    \n",
    "    return dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './DRIVING-DATASET/Detection/'\n",
    "dloaders = build_dataloader(data_dir, batch_size = 4, image_size = 448)\n",
    "\n",
    "for phase in ['train', 'val'] :\n",
    "    for index, batch in enumerate(dloaders[phase]) :\n",
    "        images = batch[0]\n",
    "        targets = batch[1]\n",
    "        filnames = batch[2]\n",
    "        print(len(filnames))\n",
    "        if index == 0 :\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 블로그에서 복사해온 모델링 코드 부분"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "backbone = torchvision.models.vgg16(pretrained=True).features[:-1]\n",
    "backbone_out = 512\n",
    "backbone.out_channels = backbone_out\n",
    "\n",
    "anchor_generator = torchvision.models.detection.rpn.AnchorGenerator(sizes=((128, 256, 512),),aspect_ratios=((0.5, 1.0, 2.0),))\n",
    "\n",
    "resolution = 7\n",
    "roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'], output_size=resolution, sampling_ratio=2)\n",
    "\n",
    "box_head = torchvision.models.detection.faster_rcnn.TwoMLPHead(in_channels= backbone_out*(resolution**2),representation_size=4096) \n",
    "box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(4096,21) #21개 class\n",
    "\n",
    "model = torchvision.models.detection.FasterRCNN(backbone, num_classes=None,\n",
    "                   min_size = 600, max_size = 1000,\n",
    "                   rpn_anchor_generator=anchor_generator,\n",
    "                   rpn_pre_nms_top_n_train = 6000, rpn_pre_nms_top_n_test = 6000,\n",
    "                   rpn_post_nms_top_n_train=2000, rpn_post_nms_top_n_test=300,\n",
    "                   rpn_nms_thresh=0.7,rpn_fg_iou_thresh=0.7,  rpn_bg_iou_thresh=0.3,\n",
    "                   rpn_batch_size_per_image=256, rpn_positive_fraction=0.5,\n",
    "                   box_roi_pool=roi_pooler, box_head = box_head, box_predictor = box_predictor,\n",
    "                   box_score_thresh=0.05, box_nms_thresh=0.7,box_detections_per_img=300,\n",
    "                   box_fg_iou_thresh=0.5, box_bg_iou_thresh=0.5,\n",
    "                   box_batch_size_per_image=128, box_positive_fraction=0.25\n",
    "                 )\n",
    "#roi head 있으면 num_class = None으로 함\n",
    "\n",
    "for param in model.rpn.parameters():\n",
    "  torch.nn.init.normal_(param,mean = 0.0, std=0.01)\n",
    "\n",
    "for name, param in model.roi_heads.named_parameters():\n",
    "  if \"bbox_pred\" in name:\n",
    "    torch.nn.init.normal_(param,mean = 0.0, std=0.001)\n",
    "  elif \"weight\" in name:\n",
    "    torch.nn.init.normal_(param,mean = 0.0, std=0.01)\n",
    "  if \"bias\" in name:\n",
    "    torch.nn.init.zeros_(param)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 코드에 관한 설명\n",
    "\n",
    "### 모델을 만드는 코드입니다. backbone으로 VGG16을 사용하며 마지막 max pooling층은 제거해 줍니다. Faster RCNN을 사용하기 위해서는 fully connected layer를 만들기 위해 최종 backbone output채널이 512임을 알려주어야 합니다.\n",
    "### 이후 anchor generator, roi pooler, box head, box predictor를 각각 만들어 줍니다.\n",
    "### box head는 Fast RCNN에서 처음 두 FC layer에 해당하는 층이고 box predictor는 예측을 하는 FC layer입니다.\n",
    "\n",
    "### 모델은 torchvision.models.detection에 있는 FasterRCNN을 사용합니다. 입력해야 하는 값을 point 에 있는 값들을 참고하여 입력하면 됩니다. default로 None인 항목 중 point에 없는 항목은 그대로 두어도 논문과 같거나 큰 영향이 없는 값들입니다.\n",
    "### 마지막으로 weight와 bias를 초기화합니다. 참고로 box_score_thresh와 box_nms_thresh는 예측때 필요한 값이므로 일단 임의의 값을 입력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor # 모델의 헤더를 수정할 수 있게 해주는 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_features = model.roi_heads.box_predictor.cls_score.in_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(num_classes) :\n",
    "    model = models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 2\n",
    "model = build_model(num_classes = NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase = 'train'\n",
    "model.train() # Sets the module in training mode.\n",
    "\n",
    "for index, batch in enumerate(dloaders[phase]) :\n",
    "    images = batch[0]\n",
    "    targets = batch[1]\n",
    "    filenames = batch[2]\n",
    "    \n",
    "    images = list(image for image in images)\n",
    "    targets = [{k : v for k, v in t.items()} for t in targets]\n",
    "    \n",
    "    loss = model(images, targets)\n",
    "    if index == 0 :\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss\n",
    "# 어떤 object인지 확인 (위에 거 두 개)\n",
    "# object가 있는지 확인 (밑에 거 두 개)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def train_one_epoch(dataloaders, model, optimizer, device) :\n",
    "    train_loss = defaultdict(float) # 딕셔너리를 만들 때 안에 들어가는 데이터들을 float으로 설정\n",
    "    val_loss = defaultdict(float)\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for phase in ['train', 'val'] :\n",
    "        for index, batch in enumerate(dataloaders[phase]) :\n",
    "            images = batch[0]\n",
    "            targets = batch[1]\n",
    "            filenames = batch[2]\n",
    "            \n",
    "            images = list(image for image in images)\n",
    "            targets = [{k : v for k, v in t.items()} for t in targets]\n",
    "            \n",
    "            with torch.set_grad_enabled(phase == 'train') :\n",
    "                loss = model(images, targets)\n",
    "            total_loss = sum(each_loss for each_loss in loss.values())\n",
    "            \n",
    "            if phase == 'train' :\n",
    "                optimizer.zero_grad()\n",
    "                total_loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                if (index > 0) and (index % VERBOSE_FREQ) == 0 :\n",
    "                    text = f\"{index}/{len(dataloaders[phase])} - \"\n",
    "                    for k, v in loss.items() :\n",
    "                        text += f\"{k}: {v.item():.4f}\"\n",
    "                    print(text)\n",
    "                    \n",
    "                for k, v in loss.items() :\n",
    "                    train_loss[k] += v.item()\n",
    "                train_loss['total_loss'] += total_loss.item()\n",
    "            \n",
    "            else :\n",
    "                for k, v in loss.items() :\n",
    "                    val_loss[k] += v.item()\n",
    "                val_loss['total_loss'] += total_loss.item()\n",
    "    \n",
    "    for k in train_loss.keys() :\n",
    "        train_loss[k] /= len(dataloaders['train'])\n",
    "        val_loss[k] /= len(dataloaders['val'])\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import save_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './DRIVING-DATASET/Detection/'\n",
    "is_cuda = True\n",
    "\n",
    "NUM_CLASSES = 2\n",
    "IMAGE_SIZE = 448\n",
    "BATCH_SIZE = 6\n",
    "VERBOSE_FREQ = 200\n",
    "# DEVICE = torch.device('cuda' if torch.cuda.is_available and is_cuda else 'cpu')\n",
    "DEVICE = 'cpu'\n",
    "dataloaders = build_dataloader(data_dir = data_dir, batch_size = BATCH_SIZE, image_size = IMAGE_SIZE)\n",
    "model = build_model(num_classes = NUM_CLASSES)\n",
    "model = model.to(DEVICE)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.001, momentum = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 30\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(1, num_epochs) :\n",
    "    \n",
    "    train_loss, val_loss = train_one_epoch(dataloaders, model, optimizer, DEVICE)\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    print(f\"{epoch+1}/{num_epochs} - Train Loss : {train_loss['total_loss']:.4f}, Val_Loss : {val_loss['total_loss']:.4f}\")\n",
    "    \n",
    "    if (epoch+1)%10 == 0 :\n",
    "        save_model(model.state_dict(), f'model_{epoch+1}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_loss_classifier = []\n",
    "tr_loss_box_reg = []\n",
    "tr_loss_objectness = []\n",
    "tr_loss_rpn_box_reg = []\n",
    "tr_loss_total = []\n",
    "for tr_loss in train_losses:\n",
    "    tr_loss_classifier.append(tr_loss['loss_classifier'])\n",
    "    tr_loss_box_reg.append(tr_loss['loss_box_reg'])\n",
    "    tr_loss_objectness.append(tr_loss['loss_objectness'])\n",
    "    tr_loss_rpn_box_reg.append(tr_loss['loss_rpn_box_reg'])\n",
    "    tr_loss_total.append(tr_loss['total_loss'])\n",
    "val_loss_classifier = []\n",
    "val_loss_box_reg = []\n",
    "val_loss_objectness = []\n",
    "val_loss_rpn_box_reg = []\n",
    "val_loss_total = []\n",
    "for vl_loss in val_losses:\n",
    "    val_loss_classifier.append(vl_loss['loss_classifier'])\n",
    "    val_loss_box_reg.append(vl_loss['loss_box_reg'])\n",
    "    val_loss_objectness.append(vl_loss['loss_objectness'])\n",
    "    val_loss_rpn_box_reg.append(vl_loss['loss_rpn_box_reg'])\n",
    "    val_loss_total.append(vl_loss['total_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(tr_loss_total, label=\"train_total_loss\")\n",
    "plt.plot(tr_loss_classifier, label=\"train_loss_classifier\")\n",
    "plt.plot(tr_loss_box_reg,  label=\"train_loss_box_reg\")\n",
    "plt.plot(tr_loss_objectness, label=\"train_loss_objectness\")\n",
    "plt.plot(tr_loss_rpn_box_reg,  label=\"train_loss_rpn_box_reg\")\n",
    "\n",
    "plt.plot(val_loss_total, label=\"val_total_loss\")\n",
    "plt.plot(val_loss_classifier, label=\"val_loss_classifier\")\n",
    "plt.plot(val_loss_box_reg,  label=\"val_loss_box_reg\")\n",
    "plt.plot(val_loss_objectness, label=\"val_loss_objectness\")\n",
    "plt.plot(val_loss_rpn_box_reg,  label=\"val_loss_rpn_box_reg\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.grid(\"on\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(ckpt_path, num_classes, device) :\n",
    "    checkpoint = torch.load(ckpt_path, map_location = device)\n",
    "    model = build_model(num_classes = num_classes)\n",
    "    model.load_state_dict(checkpoint)\n",
    "    model = model.to(device)\n",
    "    model.eval() # Sets the module in evaluation mode.\n",
    "                 # This is equivalent with self.train(False)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_cuda = True\n",
    "\n",
    "NUM_CLASSES = 2\n",
    "DEVICE = torch.device('cuda' if is_cuda and torch.cuda.is_available() else 'cpu')\n",
    "DEVICE = 'cpu'\n",
    "data_dir = './DRIVING-DATASET/Detection/'\n",
    "dataloaders = build_dataloader(data_dir, batch_size=1)\n",
    "num_classes = len(CLASS_NAME_TO_ID)\n",
    "\n",
    "model = load_model(ckpt_path='./trained_model/model_30.pth', num_classes = NUM_CLASSES, device = DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.ops import nms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(prediction, conf_thres = 0.2, IoU_threshold = 0.1) :\n",
    "    pred_box = prediction['boxes'].cpu().detach().numpy()\n",
    "    pred_label = prediction['labels'].cpu().detach().numpy()\n",
    "    pred_conf = prediction['scores'].cpu().detach().numpy()\n",
    "    \n",
    "    conf_thres = 0.2\n",
    "    valid_index = pred_conf > conf_thres\n",
    "    pred_box = pred_box[valid_index]\n",
    "    pred_label = pred_label[valid_index]\n",
    "    pred_conf = pred_conf[valid_index]\n",
    "    \n",
    "    valid_index = nms(torch.tensor(pred_box.astype(np.float32)), torch.tensor(pred_conf), IoU_threshold)\n",
    "    pred_box = pred_box[valid_index.numpy()]\n",
    "    pred_conf = pred_conf[valid_index.numpy()]\n",
    "    pred_label = pred_label[valid_index.numpy()]\n",
    "    \n",
    "    return np.concatenate((pred_box, pred_conf[:, np.newaxis], pred_label[:, np.newaxis]), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_images = []\n",
    "pred_labels =[]\n",
    "for index, (images, _, filenames) in enumerate(dataloaders[\"val\"]):\n",
    "    images = list(image.to(DEVICE) for image in images)\n",
    "    filename = filenames[0]\n",
    "    image = make_grid(images[0].cpu().detach(), normalize=True).permute(1,2,0).numpy()\n",
    "    image = (image * 255).astype(np.uint8)\n",
    "    with torch.no_grad():\n",
    "        prediction = model(images)\n",
    "    prediction = postprocess(prediction[0])\n",
    "    prediction[:, 2].clip(min=0, max=image.shape[1])\n",
    "    prediction[:, 3].clip(min=0, max=image.shape[0])\n",
    "    xc = (prediction[:, 0] + prediction[:, 2])/2\n",
    "    yc = (prediction[:, 1] + prediction[:, 3])/2\n",
    "    w = prediction[:, 2] - prediction[:, 0]\n",
    "    h = prediction[:, 3] - prediction[:, 1]\n",
    "    cls_id = prediction[:, 5]\n",
    "    prediction_yolo = np.stack([xc,yc, w,h, cls_id], axis=1)\n",
    "    pred_images.append(image)\n",
    "    pred_labels.append(prediction_yolo)\n",
    "    if index == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 테스트 부분 일단 복사만 해옴 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = './DRIVING-DATASET/sample_video.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize(size=(IMAGE_SIZE, IMAGE_SIZE)),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "@torch.no_grad()\n",
    "def model_predict(image, model):\n",
    "    tensor_image = transformer(image)\n",
    "    tensor_image = tensor_image.to(DEVICE)\n",
    "    prediction = model([tensor_image])\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid = cv2.VideoCapture(video_path)\n",
    "while (vid.isOpened()):\n",
    "    ret, frame = vid.read()\n",
    "    if ret:\n",
    "        since = time()\n",
    "        ori_h, ori_w = frame.shape[:2]\n",
    "        image = cv2.resize(frame, dsize=(IMAGE_SIZE, IMAGE_SIZE))\n",
    "        prediction = model_predict(image, model)\n",
    "        prediction = postprocess(prediction[0])\n",
    "        prediction[:, [0,2]] *= (ori_w/IMAGE_SIZE)\n",
    "        prediction[:, [1,3]] *= (ori_h/IMAGE_SIZE)\n",
    "        prediction[:, 2].clip(min=0, max=ori_w)\n",
    "        prediction[:, 3].clip(min=0, max=ori_h)\n",
    "        xc = (prediction[:, 0] + prediction[:, 2])/2\n",
    "        yc = (prediction[:, 1] + prediction[:, 3])/2\n",
    "        w = prediction[:, 2] - prediction[:, 0]\n",
    "        h = prediction[:, 3] - prediction[:, 1]\n",
    "        cls_id = prediction[:, 5]\n",
    "        prediction_yolo = np.stack([xc,yc, w,h, cls_id], axis=1)\n",
    "        text= f\"{(time() - since)*1000:.0f}ms/image\"\n",
    "        canvas = visualize(frame, prediction_yolo[:, 0:4], prediction_yolo[:, 4])\n",
    "        cv2.putText(canvas, text, (20, 40), cv2.FONT_HERSHEY_PLAIN, 2, (255, 255, 255), 2)\n",
    "        cv2.imshow('camera', canvas)\n",
    "        key = cv2.waitKey(1)\n",
    "        if key == 27:\n",
    "            break\n",
    "        if key == ord('s'):\n",
    "            cv2.waitKey()\n",
    "vid.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
